{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('train.csv')\n",
    "Y_LABEL = 'y'                                   \t\t\t        # Name of the variable to be predicted\n",
    "KEYS = [i for i in raw_data.keys().tolist() if i != Y_LABEL]\t# Name of predictors\n",
    "N_INSTANCES = raw_data.shape[0]                     \t\t\t    # Number of instances\n",
    "N_INPUT = raw_data.shape[1] - 1                     \t\t\t    # Input size\n",
    "N_CLASSES = raw_data[Y_LABEL].unique().shape[0]     \t\t\t    # Number of classes (output size)\n",
    "TEST_SIZE = 0.1                                    \t\t\t      # Test set size (% of dataset)\n",
    "TRAIN_SIZE = int(N_INSTANCES * (1 - TEST_SIZE))     \t\t\t    # Train size\n",
    "LEARNING_RATE = 0.01                               \t\t\t    # Learning rate\n",
    "TRAINING_EPOCHS = 400                               \t\t\t    # Number of epochs\n",
    "BATCH_SIZE = 100                                    \t\t\t    # Batch size\n",
    "DISPLAY_STEP = 20                                    \t\t\t    # Display progress each x epochs\n",
    "HIDDEN_SIZE = [128, 128, 64, 32]\t                                   \t\t      # Number of hidden neurons 256\n",
    "ACTIVATION_FUNCTION_OUT = tf.nn.softmax                          # Last layer act fct\n",
    "STDDEV = 0.1                                        \t\t\t    # Standard deviation (for weights random init)\n",
    "RANDOM_STATE = 100\t\t\t\t\t\t\t\t                            # Random state for train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = raw_data[KEYS].get_values()                  \t\t\t# X data\n",
    "labels = raw_data[Y_LABEL].get_values()  \n",
    "labels_ = np.zeros((N_INSTANCES, N_CLASSES))\n",
    "labels_[np.arange(N_INSTANCES), labels.astype(np.int0)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test, labels_train, labels_test = train_test_split(data,\n",
    "                                                                    labels_,\n",
    "                                                                    test_size = TEST_SIZE,\n",
    "                                                                    random_state = RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Net params\n",
    "n_input = N_INPUT                   # input n labels\n",
    "n_hidden_1 = HIDDEN_SIZE[0]            # 1st layer\n",
    "n_hidden_2 = HIDDEN_SIZE[1]            # 2nd layer\n",
    "n_hidden_3 = HIDDEN_SIZE[2]            # 3rd layer\n",
    "n_hidden_4 = HIDDEN_SIZE[3]            # 4th layer\n",
    "n_classes = N_CLASSES               # output m classes\n",
    "\n",
    "# regularization params\n",
    "lambda_1 = .15\n",
    "lambda_2 = .1\n",
    "\n",
    "alpha_1 = .5\n",
    "alpha_2 = .15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "dropout_keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "def mlp(_X, _weights, _biases, dropout_keep_prob):\n",
    "    weighted_layer = tf.multiply(_weights['W'], _X)\n",
    "    layer1 = tf.nn.dropout(tf.nn.relu(tf.add(tf.matmul(weighted_layer, _weights['h1']), _biases['b1'])), dropout_keep_prob)\n",
    "    layer2 = tf.nn.dropout(tf.nn.relu(tf.add(tf.matmul(layer1, _weights['h2']), _biases['b2'])), dropout_keep_prob)\n",
    "    layer3 = tf.nn.dropout(tf.nn.relu(tf.add(tf.matmul(layer2, _weights['h3']), _biases['b3'])), dropout_keep_prob)\n",
    "    layer4 = tf.nn.dropout(tf.nn.relu(tf.add(tf.matmul(layer3, _weights['h4']), _biases['b4'])), dropout_keep_prob)\n",
    "    out = ACTIVATION_FUNCTION_OUT(tf.add(tf.matmul(layer4, _weights['out']), _biases['out']))\n",
    "    return out\n",
    "\n",
    "weights = {\n",
    "    'W': tf.Variable(tf.random_normal([n_input], stddev=STDDEV)),\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1], stddev=STDDEV)),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2], stddev=STDDEV)),\n",
    "    'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3], stddev=STDDEV)),\n",
    "    'h4': tf.Variable(tf.random_normal([n_hidden_3, n_hidden_4], stddev=STDDEV)),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_4, n_classes], stddev=STDDEV)),                                   \n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'b3': tf.Variable(tf.random_normal([n_hidden_3])),\n",
    "    'b4': tf.Variable(tf.random_normal([n_hidden_4])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "\n",
    "}\n",
    "hidden_weights = ['h1', 'h2', 'h3', 'h4', 'out']\n",
    "\n",
    "# Build model\n",
    "pred = mlp(X, weights, biases, dropout_keep_prob)\n",
    "\n",
    "# Loss, regularization and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred, labels=y)) # softmax loss\n",
    "\n",
    "regularizers = lambda_1*(((1-lambda_2)/2) * tf.nn.l2_loss(weights['W']) \\\n",
    "                         + lambda_2*tf.sqrt(tf.nn.l2_loss(weights['W']))) + \\\n",
    "alpha_1 * ((1 - alpha_2)/2 * tf.reduce_sum([tf.nn.l2_loss(weights[hidden]) \\\n",
    "                                            for hidden in hidden_weights]) + \\\n",
    "           alpha_2*tf.reduce_sum([tf.sqrt(tf.nn.l2_loss(weights[hidden])) for hidden in hidden_weights]))\n",
    "cost += regularizers\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE).minimize(cost, var_list=tf.trainable_variables())\n",
    "\n",
    "# Accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "auc = tf.metrics.auc(tf.argmax(y, 1), tf.argmax(pred, 1), curve='ROC')\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000/400 cost: 113.562293582\n",
      "Training AUC: (0.0, 0.49999997)\n",
      "Epoch: 020/400 cost: 0.575876468\n",
      "Training AUC: (0.49999997, 0.5)\n",
      "Epoch: 040/400 cost: 0.591606994\n",
      "Training AUC: (0.5, 0.5)\n",
      "Epoch: 060/400 cost: 0.593868223\n",
      "Training AUC: (0.5, 0.5)\n",
      "Epoch: 080/400 cost: 0.616684894\n",
      "Training AUC: (0.5, 0.5)\n",
      "Epoch: 100/400 cost: 0.579788496\n",
      "Training AUC: (0.5, 0.5)\n",
      "Epoch: 120/400 cost: 0.610240916\n",
      "Training AUC: (0.5, 0.5)\n",
      "Epoch: 140/400 cost: 0.580692808\n",
      "Training AUC: (0.5, 0.5)\n",
      "Epoch: 160/400 cost: 0.594021930\n",
      "Training AUC: (0.5, 0.5)\n",
      "Epoch: 180/400 cost: 0.576585743\n",
      "Training AUC: (0.5, 0.5)\n",
      "Epoch: 200/400 cost: 0.612409406\n",
      "Training AUC: (0.5, 0.5)\n",
      "Epoch: 220/400 cost: 0.595846580\n",
      "Training AUC: (0.5, 0.5)\n",
      "Epoch: 240/400 cost: 0.596026937\n",
      "Training AUC: (0.5, 0.5)\n",
      "Epoch: 260/400 cost: 0.601577169\n",
      "Training AUC: (0.5, 0.5)\n",
      "Epoch: 280/400 cost: 0.615422997\n",
      "Training AUC: (0.5, 0.5)\n",
      "Epoch: 300/400 cost: 0.596251574\n",
      "Training AUC: (0.5, 0.5)\n",
      "Epoch: 320/400 cost: 0.584533248\n",
      "Training AUC: (0.5, 0.5)\n",
      "Epoch: 340/400 cost: 0.591995656\n",
      "Training AUC: (0.5, 0.5)\n",
      "Epoch: 360/400 cost: 0.600711458\n",
      "Training AUC: (0.5, 0.5)\n",
      "Epoch: 380/400 cost: 0.606260277\n",
      "Training AUC: (0.5, 0.5)\n"
     ]
    }
   ],
   "source": [
    "# Initialize variables\n",
    "init_all = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "\n",
    "# Launch session\n",
    "sess = tf.Session()\n",
    "sess.run(init_all)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(TRAINING_EPOCHS):\n",
    "    avg_cost = 0.\n",
    "    total_batch = int(data_train.shape[0] / BATCH_SIZE)\n",
    "    # Loop over all batches\n",
    "    for i in range(total_batch):\n",
    "        randidx = np.random.randint(int(TRAIN_SIZE), size=BATCH_SIZE)\n",
    "        batch_xs = data_train[randidx, :]\n",
    "        batch_ys = labels_train[randidx]\n",
    "        # Fit using batched data\n",
    "        sess.run(optimizer, feed_dict={X: batch_xs, y: batch_ys, dropout_keep_prob: 0.9})\n",
    "        # Calculate average cost\n",
    "        avg_cost += sess.run(cost, feed_dict={X: batch_xs, y: batch_ys, dropout_keep_prob:1.})/total_batch\n",
    "    # Display progress\n",
    "    if epoch % DISPLAY_STEP == 0:\n",
    "        print (f\"Epoch: {epoch:03d}/{TRAINING_EPOCHS:03d} cost: {avg_cost:.9f}\")\n",
    "        train_auc = sess.run(auc, feed_dict={X: batch_xs, y: batch_ys, dropout_keep_prob:1.})\n",
    "\n",
    "        print (f\"Training AUC: {train_auc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_auc = sess.run(auc, feed_dict={X: data_test, y: labels_test, dropout_keep_prob:1.})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
